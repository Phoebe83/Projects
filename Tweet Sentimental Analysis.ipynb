{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>ANLY 580 Project 1</H1>\n",
    "<H3>Tweet Sentiment Analysis</H3>\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.sentiment.util import *\n",
    "from collections import Counter, OrderedDict \n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import regex\n",
    "import string\n",
    "import statistics \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Part 1: Tweet Data Exploratory Data Analysis</h3>\n",
    "<br>The tweets data are collected from four txt files, each contains tweet's ID, content and the label (positive/neutral/negative)</br>\n",
    "<br>For training purposes, the ID of each tweet is irrelevant for predicting the polarity of the tweet so that will not be collected</br>\n",
    "<br>In this part, the input tweet texts are cleaned basically from stop words, punctuations, lower case and casual_tokenize function from NLTK library for displaying basic information purposes. The following modeling part will have more cleaning methods for training purposes.</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset\n",
    "reviews_input = []\n",
    "with open('INPUT.txt', 'r', encoding=\"utf8\") as file:\n",
    "    reviews_input.append(list(zip(*(line.strip().split('\\t') for line in file))))\n",
    "    \n",
    "reviews_full = []\n",
    "with open('train.txt', 'r', encoding=\"utf8\") as file:\n",
    "    reviews_full.append(list(zip(*(line.strip().split('\\t') for line in file))))\n",
    "    \n",
    "with open('dev.txt', 'r', encoding=\"utf8\") as file:\n",
    "    reviews_full.append(list(zip(*(line.strip().split('\\t') for line in file))))\n",
    "    \n",
    "with open('devtest.txt', 'r', encoding=\"utf8\") as file:\n",
    "    reviews_full.append(list(zip(*(line.strip().split('\\t') for line in file))))\n",
    "\n",
    "with open('test.txt', 'r', encoding=\"utf8\") as file:\n",
    "    reviews_full.append(list(zip(*(line.strip().split('\\t') for line in file))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the content and labels of tweets\n",
    "reviews_train = reviews_full[0][2] + reviews_full[1][2] + reviews_full[2][2] + reviews_full[3][2]\n",
    "reviews_label = reviews_full[0][1] + reviews_full[1][1] + reviews_full[2][1] + reviews_full[3][1]\n",
    "\n",
    "#Input.txt\n",
    "reviews_train_input = reviews_input[0][2]\n",
    "reviews_label_input = reviews_input[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'text':reviews_train, 'label':reviews_label})\n",
    "df_input = pd.DataFrame({'text':reviews_train_input, 'label':reviews_label_input})\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Input.txt</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#ArianaGrande Ari By Ariana Grande 80% Full ht...</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ariana Grande KIIS FM Yours Truly CD listening...</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ariana Grande White House Easter Egg Roll in W...</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#CD #Musics Ariana Grande Sweet Like Candy 3.4...</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SIDE TO SIDE üòò @arianagrande #sidetoside #aria...</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hairspray Live! Previews at the Macy's Thanksg...</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#LindsayLohan Is ‚ÄòFeeling Thankful‚Äô After Blas...</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I hate her but... I love her songs Dammit ._.#...</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ariana Grande „ÄêRight There ft. Big Sean„Äë#„Ç¢„É™„Ç¢„Éä ...</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>which one would you prefer to listen to for a ...</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    label\n",
       "0  #ArianaGrande Ari By Ariana Grande 80% Full ht...  UNKNOWN\n",
       "1  Ariana Grande KIIS FM Yours Truly CD listening...  UNKNOWN\n",
       "2  Ariana Grande White House Easter Egg Roll in W...  UNKNOWN\n",
       "3  #CD #Musics Ariana Grande Sweet Like Candy 3.4...  UNKNOWN\n",
       "4  SIDE TO SIDE üòò @arianagrande #sidetoside #aria...  UNKNOWN\n",
       "5  Hairspray Live! Previews at the Macy's Thanksg...  UNKNOWN\n",
       "6  #LindsayLohan Is ‚ÄòFeeling Thankful‚Äô After Blas...  UNKNOWN\n",
       "7  I hate her but... I love her songs Dammit ._.#...  UNKNOWN\n",
       "8  Ariana Grande „ÄêRight There ft. Big Sean„Äë#„Ç¢„É™„Ç¢„Éä ...  UNKNOWN\n",
       "9  which one would you prefer to listen to for a ...  UNKNOWN"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Glance the first ten rows of the raw data in table format\n",
    "df_input.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Data Cleaning before EDA </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove punctuation and stop words, make all tokens lower-case\n",
    "punctuation = list(string.punctuation) + ['‚Ä¶', '...','‚Äô']\n",
    "stop_words = stopwords.words('english') + ['rt', 'via']\n",
    "\n",
    "tweet_tokens = []\n",
    "corpus = []\n",
    "for tweet in df_input['text']:\n",
    "    tokens = nltk.casual_tokenize(tweet)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    tokens = [token for token in tokens if not token in stop_words]\n",
    "    tokens = [token for token in tokens if not token in punctuation]\n",
    "    tweet_tokens.append(tokens)\n",
    "    corpus.extend(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>Total number of tweets</H4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12284 number of tweets for input.txt.\n"
     ]
    }
   ],
   "source": [
    "print('There are', df_input.shape[0], 'number of tweets for input.txt.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>Total number of characters</H4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters for 12284 number of tweets is: 961357\n"
     ]
    }
   ],
   "source": [
    "# The number of words, characters and av char count for each tweet\n",
    "def tweet_counts(tweet):\n",
    "    char_counts = [len(word) for word in tweet]\n",
    "    total_chars = sum(char_counts)\n",
    "    av_chars = total_chars / len(char_counts)\n",
    "    return len(tweet), total_chars, av_chars\n",
    "\n",
    "all_char = []\n",
    "for tweet in tweet_tokens:\n",
    "    all_char.append(tweet_counts(tweet)[1])\n",
    "print('Total number of characters for',  df_input.shape[0], 'number of tweets is:', sum(all_char))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>Total number of distinct words (vocabulary)</H4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of distinct words for 12284 number of tweets is: 35300\n"
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "for tweet in tweet_tokens:\n",
    "    for word in tweet:\n",
    "        all_words.append(word.lower())\n",
    "distinct_words = set(all_words)\n",
    "print('Total number of distinct words for',  df_input.shape[0], 'number of tweets is:', len(distinct_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>Average number of characters and words in each tweet</H4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of characters per word for each tweet is listed below:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[9.5,\n",
       " 7.454545454545454,\n",
       " 8.1,\n",
       " 6.444444444444445,\n",
       " 8.583333333333334,\n",
       " 9.8,\n",
       " 10.181818181818182,\n",
       " 6.4,\n",
       " 6.2727272727272725,\n",
       " 5.294117647058823,\n",
       " 11.555555555555555,\n",
       " 8.909090909090908,\n",
       " 10.25,\n",
       " 11.25,\n",
       " 11.222222222222221,\n",
       " 10.818181818181818,\n",
       " 5.266666666666667,\n",
       " 8.0,\n",
       " 11.444444444444445,\n",
       " 6.714285714285714,\n",
       " 13.0,\n",
       " 6.875,\n",
       " 8.285714285714286,\n",
       " 7.666666666666667,\n",
       " 9.375,\n",
       " 9.25,\n",
       " 9.416666666666666,\n",
       " 6.866666666666666,\n",
       " 11.375,\n",
       " 8.166666666666666,\n",
       " 11.5,\n",
       " 7.636363636363637,\n",
       " 9.23076923076923,\n",
       " 8.222222222222221,\n",
       " 7.5,\n",
       " 9.625,\n",
       " 9.818181818181818,\n",
       " 5.733333333333333,\n",
       " 9.222222222222221,\n",
       " 11.727272727272727,\n",
       " 7.615384615384615,\n",
       " 13.0,\n",
       " 9.666666666666666,\n",
       " 5.722222222222222,\n",
       " 8.833333333333334,\n",
       " 9.25,\n",
       " 8.5,\n",
       " 7.571428571428571,\n",
       " 9.222222222222221,\n",
       " 11.666666666666666,\n",
       " 8.25,\n",
       " 7.384615384615385,\n",
       " 10.9,\n",
       " 13.2,\n",
       " 8.666666666666666,\n",
       " 10.777777777777779,\n",
       " 10.3,\n",
       " 6.117647058823529,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 5.923076923076923,\n",
       " 8.0,\n",
       " 10.5,\n",
       " 5.7,\n",
       " 6.916666666666667,\n",
       " 11.444444444444445,\n",
       " 8.25,\n",
       " 10.75,\n",
       " 8.142857142857142,\n",
       " 7.545454545454546,\n",
       " 7.066666666666666,\n",
       " 9.7,\n",
       " 6.875,\n",
       " 7.8,\n",
       " 8.76923076923077,\n",
       " 9.125,\n",
       " 10.9,\n",
       " 10.545454545454545,\n",
       " 10.875,\n",
       " 10.454545454545455,\n",
       " 9.875,\n",
       " 8.076923076923077,\n",
       " 8.0,\n",
       " 10.0,\n",
       " 7.230769230769231,\n",
       " 8.0,\n",
       " 11.0,\n",
       " 6.538461538461538,\n",
       " 11.666666666666666,\n",
       " 7.153846153846154,\n",
       " 11.285714285714286,\n",
       " 8.0,\n",
       " 11.3,\n",
       " 6.916666666666667,\n",
       " 10.2,\n",
       " 9.909090909090908,\n",
       " 10.583333333333334,\n",
       " 5.444444444444445,\n",
       " 14.0,\n",
       " 9.25,\n",
       " 6.125,\n",
       " 11.714285714285714,\n",
       " 5.538461538461538,\n",
       " 5.7272727272727275,\n",
       " 5.533333333333333,\n",
       " 7.4,\n",
       " 5.923076923076923,\n",
       " 4.636363636363637,\n",
       " 6.444444444444445,\n",
       " 7.1875,\n",
       " 7.333333333333333,\n",
       " 9.166666666666666,\n",
       " 9.428571428571429,\n",
       " 4.0,\n",
       " 9.142857142857142,\n",
       " 11.0,\n",
       " 6.5,\n",
       " 5.5,\n",
       " 7.454545454545454,\n",
       " 7.083333333333333,\n",
       " 6.461538461538462,\n",
       " 7.9,\n",
       " 8.6,\n",
       " 8.083333333333334,\n",
       " 4.8,\n",
       " 8.666666666666666,\n",
       " 7.777777777777778,\n",
       " 6.466666666666667,\n",
       " 6.4375,\n",
       " 12.6,\n",
       " 6.833333333333333,\n",
       " 8.285714285714286,\n",
       " 6.916666666666667,\n",
       " 5.666666666666667,\n",
       " 8.666666666666666,\n",
       " 8.214285714285714,\n",
       " 6.6923076923076925,\n",
       " 4.8,\n",
       " 8.714285714285714,\n",
       " 10.571428571428571,\n",
       " 5.916666666666667,\n",
       " 7.5,\n",
       " 9.11111111111111,\n",
       " 4.583333333333333,\n",
       " 7.333333333333333,\n",
       " 8.571428571428571,\n",
       " 4.642857142857143,\n",
       " 7.0,\n",
       " 7.5,\n",
       " 8.571428571428571,\n",
       " 7.375,\n",
       " 11.125,\n",
       " 7.3076923076923075,\n",
       " 4.777777777777778,\n",
       " 8.0,\n",
       " 6.0,\n",
       " 8.666666666666666,\n",
       " 6.625,\n",
       " 8.666666666666666,\n",
       " 7.0,\n",
       " 9.857142857142858,\n",
       " 6.083333333333333,\n",
       " 7.888888888888889,\n",
       " 6.615384615384615,\n",
       " 9.375,\n",
       " 7.375,\n",
       " 7.071428571428571,\n",
       " 8.222222222222221,\n",
       " 5.9375,\n",
       " 7.166666666666667,\n",
       " 10.8,\n",
       " 7.923076923076923,\n",
       " 7.6,\n",
       " 7.888888888888889,\n",
       " 5.533333333333333,\n",
       " 6.166666666666667,\n",
       " 5.5,\n",
       " 6.928571428571429,\n",
       " 9.4,\n",
       " 6.833333333333333,\n",
       " 5.1875,\n",
       " 10.375,\n",
       " 6.642857142857143,\n",
       " 5.055555555555555,\n",
       " 6.363636363636363,\n",
       " 7.0,\n",
       " 9.285714285714286,\n",
       " 7.555555555555555,\n",
       " 8.428571428571429,\n",
       " 12.666666666666666,\n",
       " 5.7272727272727275,\n",
       " 8.777777777777779,\n",
       " 4.666666666666667,\n",
       " 9.833333333333334,\n",
       " 7.5,\n",
       " 9.666666666666666,\n",
       " 9.363636363636363,\n",
       " 7.916666666666667,\n",
       " 9.4,\n",
       " 6.6,\n",
       " 5.5,\n",
       " 5.0,\n",
       " 7.833333333333333,\n",
       " 5.3,\n",
       " 6.0,\n",
       " 11.6,\n",
       " 6.142857142857143,\n",
       " 6.111111111111111,\n",
       " 5.933333333333334,\n",
       " 5.5,\n",
       " 5.181818181818182,\n",
       " 7.333333333333333,\n",
       " 4.642857142857143,\n",
       " 5.0,\n",
       " 20.0,\n",
       " 14.6,\n",
       " 4.454545454545454,\n",
       " 8.416666666666666,\n",
       " 4.2727272727272725,\n",
       " 5.142857142857143,\n",
       " 7.666666666666667,\n",
       " 6.6,\n",
       " 4.333333333333333,\n",
       " 7.4,\n",
       " 5.083333333333333,\n",
       " 4.916666666666667,\n",
       " 10.8,\n",
       " 6.5,\n",
       " 6.846153846153846,\n",
       " 8.583333333333334,\n",
       " 5.6,\n",
       " 9.076923076923077,\n",
       " 4.416666666666667,\n",
       " 6.454545454545454,\n",
       " 5.666666666666667,\n",
       " 9.222222222222221,\n",
       " 12.88888888888889,\n",
       " 7.0,\n",
       " 5.125,\n",
       " 3.9,\n",
       " 20.0,\n",
       " 4.0,\n",
       " 7.166666666666667,\n",
       " 6.8,\n",
       " 5.333333333333333,\n",
       " 4.526315789473684,\n",
       " 6.666666666666667,\n",
       " 7.7272727272727275,\n",
       " 8.875,\n",
       " 4.545454545454546,\n",
       " 7.0,\n",
       " 4.894736842105263,\n",
       " 7.6923076923076925,\n",
       " 11.25,\n",
       " 6.461538461538462,\n",
       " 7.3,\n",
       " 6.818181818181818,\n",
       " 4.142857142857143,\n",
       " 6.6,\n",
       " 4.333333333333333,\n",
       " 5.0,\n",
       " 10.125,\n",
       " 4.875,\n",
       " 5.636363636363637,\n",
       " 5.818181818181818,\n",
       " 5.222222222222222,\n",
       " 4.5,\n",
       " 10.333333333333334,\n",
       " 7.2,\n",
       " 6.3076923076923075,\n",
       " 6.5,\n",
       " 4.333333333333333,\n",
       " 7.0,\n",
       " 4.888888888888889,\n",
       " 5.411764705882353,\n",
       " 5.785714285714286,\n",
       " 5.0,\n",
       " 4.4,\n",
       " 5.888888888888889,\n",
       " 5.0,\n",
       " 7.0,\n",
       " 4.416666666666667,\n",
       " 4.666666666666667,\n",
       " 4.5,\n",
       " 4.583333333333333,\n",
       " 4.777777777777778,\n",
       " 5.133333333333334,\n",
       " 9.125,\n",
       " 4.8,\n",
       " 4.785714285714286,\n",
       " 5.055555555555555,\n",
       " 5.090909090909091,\n",
       " 6.888888888888889,\n",
       " 5.35,\n",
       " 8.0,\n",
       " 4.545454545454546,\n",
       " 5.529411764705882,\n",
       " 5.666666666666667,\n",
       " 6.133333333333334,\n",
       " 7.4,\n",
       " 5.714285714285714,\n",
       " 4.352941176470588,\n",
       " 7.4375,\n",
       " 8.4,\n",
       " 12.333333333333334,\n",
       " 9.833333333333334,\n",
       " 5.8,\n",
       " 8.538461538461538,\n",
       " 7.666666666666667,\n",
       " 9.3,\n",
       " 8.181818181818182,\n",
       " 6.133333333333334,\n",
       " 9.0,\n",
       " 9.555555555555555,\n",
       " 8.214285714285714,\n",
       " 7.333333333333333,\n",
       " 9.625,\n",
       " 8.727272727272727,\n",
       " 6.235294117647059,\n",
       " 6.4,\n",
       " 9.0,\n",
       " 6.666666666666667,\n",
       " 9.142857142857142,\n",
       " 6.666666666666667,\n",
       " 6.277777777777778,\n",
       " 6.375,\n",
       " 6.533333333333333,\n",
       " 7.8,\n",
       " 8.416666666666666,\n",
       " 8.625,\n",
       " 10.444444444444445,\n",
       " 6.588235294117647,\n",
       " 8.666666666666666,\n",
       " 6.777777777777778,\n",
       " 7.181818181818182,\n",
       " 6.0,\n",
       " 8.0,\n",
       " 6.75,\n",
       " 6.714285714285714,\n",
       " 8.75,\n",
       " 6.8,\n",
       " 7.583333333333333,\n",
       " 7.3076923076923075,\n",
       " 8.333333333333334,\n",
       " 8.454545454545455,\n",
       " 9.909090909090908,\n",
       " 8.142857142857142,\n",
       " 5.4,\n",
       " 7.363636363636363,\n",
       " 10.0,\n",
       " 6.909090909090909,\n",
       " 6.083333333333333,\n",
       " 6.846153846153846,\n",
       " 9.916666666666666,\n",
       " 8.6,\n",
       " 8.666666666666666,\n",
       " 8.166666666666666,\n",
       " 8.2,\n",
       " 9.25,\n",
       " 5.368421052631579,\n",
       " 6.923076923076923,\n",
       " 8.0,\n",
       " 9.5,\n",
       " 8.615384615384615,\n",
       " 9.333333333333334,\n",
       " 9.0,\n",
       " 8.363636363636363,\n",
       " 8.083333333333334,\n",
       " 6.846153846153846,\n",
       " 14.777777777777779,\n",
       " 11.909090909090908,\n",
       " 16.625,\n",
       " 7.2727272727272725,\n",
       " 11.363636363636363,\n",
       " 7.090909090909091,\n",
       " 10.0,\n",
       " 12.6,\n",
       " 8.538461538461538,\n",
       " 10.125,\n",
       " 9.666666666666666,\n",
       " 9.2,\n",
       " 8.25,\n",
       " 8.5,\n",
       " 7.142857142857143,\n",
       " 10.0,\n",
       " 8.636363636363637,\n",
       " 10.0,\n",
       " 5.833333333333333,\n",
       " 7.363636363636363,\n",
       " 8.75,\n",
       " 7.428571428571429,\n",
       " 8.307692307692308,\n",
       " 9.428571428571429,\n",
       " 8.909090909090908,\n",
       " 7.916666666666667,\n",
       " 9.583333333333334,\n",
       " 10.2,\n",
       " 8.727272727272727,\n",
       " 8.25,\n",
       " 9.6,\n",
       " 11.0,\n",
       " 10.333333333333334,\n",
       " 5.5,\n",
       " 9.11111111111111,\n",
       " 11.285714285714286,\n",
       " 8.76923076923077,\n",
       " 10.0,\n",
       " 5.375,\n",
       " 10.333333333333334,\n",
       " 8.0,\n",
       " 7.833333333333333,\n",
       " 6.5,\n",
       " 7.714285714285714,\n",
       " 7.909090909090909,\n",
       " 8.7,\n",
       " 7.857142857142857,\n",
       " 8.375,\n",
       " 8.333333333333334,\n",
       " 9.636363636363637,\n",
       " 6.9,\n",
       " 7.3076923076923075,\n",
       " 9.166666666666666,\n",
       " 6.909090909090909,\n",
       " 6.923076923076923,\n",
       " 12.2,\n",
       " 6.333333333333333,\n",
       " 10.0,\n",
       " 7.833333333333333,\n",
       " 9.0,\n",
       " 8.833333333333334,\n",
       " 7.777777777777778,\n",
       " 5.181818181818182,\n",
       " 7.636363636363637,\n",
       " 8.0,\n",
       " 9.166666666666666,\n",
       " 8.0,\n",
       " 7.071428571428571,\n",
       " 9.375,\n",
       " 7.615384615384615,\n",
       " 8.333333333333334,\n",
       " 11.11111111111111,\n",
       " 8.727272727272727,\n",
       " 9.9,\n",
       " 5.9375,\n",
       " 10.375,\n",
       " 7.833333333333333,\n",
       " 5.777777777777778,\n",
       " 4.25,\n",
       " 7.615384615384615,\n",
       " 8.454545454545455,\n",
       " 7.444444444444445,\n",
       " 12.571428571428571,\n",
       " 8.0,\n",
       " 8.375,\n",
       " 5.75,\n",
       " 8.555555555555555,\n",
       " 8.333333333333334,\n",
       " 7.222222222222222,\n",
       " 7.333333333333333,\n",
       " 8.9,\n",
       " 9.0,\n",
       " 7.3076923076923075,\n",
       " 8.538461538461538,\n",
       " 6.583333333333333,\n",
       " 7.818181818181818,\n",
       " 7.636363636363637,\n",
       " 6.428571428571429,\n",
       " 7.0,\n",
       " 5.5,\n",
       " 9.0,\n",
       " 6.916666666666667,\n",
       " 8.166666666666666,\n",
       " 6.9,\n",
       " 8.2,\n",
       " 7.833333333333333,\n",
       " 8.0,\n",
       " 6.909090909090909,\n",
       " 6.583333333333333,\n",
       " 6.714285714285714,\n",
       " 8.6,\n",
       " 5.625,\n",
       " 6.8,\n",
       " 9.166666666666666,\n",
       " 7.571428571428571,\n",
       " 7.888888888888889,\n",
       " 7.090909090909091,\n",
       " 6.2,\n",
       " 10.666666666666666,\n",
       " 8.636363636363637,\n",
       " 6.7272727272727275,\n",
       " 7.25,\n",
       " 7.230769230769231,\n",
       " 7.5,\n",
       " 10.11111111111111,\n",
       " 6.7,\n",
       " 6.214285714285714,\n",
       " 7.857142857142857,\n",
       " 9.0,\n",
       " 8.0,\n",
       " 7.833333333333333,\n",
       " 6.75,\n",
       " 11.666666666666666,\n",
       " 7.538461538461538,\n",
       " 6.4375,\n",
       " 8.583333333333334,\n",
       " 10.5,\n",
       " 9.88888888888889,\n",
       " 8.333333333333334,\n",
       " 8.2,\n",
       " 8.666666666666666,\n",
       " 8.666666666666666,\n",
       " 6.666666666666667,\n",
       " 8.0,\n",
       " 8.454545454545455,\n",
       " 7.545454545454546,\n",
       " 7.5,\n",
       " 10.8,\n",
       " 7.6,\n",
       " 8.363636363636363,\n",
       " 5.894736842105263,\n",
       " 7.714285714285714,\n",
       " 6.0,\n",
       " 9.2,\n",
       " 6.6923076923076925,\n",
       " 8.181818181818182,\n",
       " 10.545454545454545,\n",
       " 7.636363636363637,\n",
       " 7.285714285714286,\n",
       " 7.153846153846154,\n",
       " 7.666666666666667,\n",
       " 6.666666666666667,\n",
       " 8.583333333333334,\n",
       " 8.545454545454545,\n",
       " 10.545454545454545,\n",
       " 6.5625,\n",
       " 8.666666666666666,\n",
       " 6.5,\n",
       " 6.0,\n",
       " 8.909090909090908,\n",
       " 6.444444444444445,\n",
       " 7.857142857142857,\n",
       " 7.928571428571429,\n",
       " 7.714285714285714,\n",
       " 8.75,\n",
       " 6.0,\n",
       " 7.846153846153846,\n",
       " 10.9,\n",
       " 10.0,\n",
       " 10.88888888888889,\n",
       " 6.8,\n",
       " 8.916666666666666,\n",
       " 6.875,\n",
       " 7.461538461538462,\n",
       " 7.333333333333333,\n",
       " 6.5625,\n",
       " 5.647058823529412,\n",
       " 7.083333333333333,\n",
       " 12.0,\n",
       " 9.538461538461538,\n",
       " 7.714285714285714,\n",
       " 7.428571428571429,\n",
       " 7.6,\n",
       " 9.153846153846153,\n",
       " 8.75,\n",
       " 10.222222222222221,\n",
       " 7.7,\n",
       " 6.933333333333334,\n",
       " 7.769230769230769,\n",
       " 10.8,\n",
       " 6.583333333333333,\n",
       " 8.818181818181818,\n",
       " 6.071428571428571,\n",
       " 7.0,\n",
       " 6.466666666666667,\n",
       " 6.571428571428571,\n",
       " 11.444444444444445,\n",
       " 7.461538461538462,\n",
       " 8.461538461538462,\n",
       " 9.0,\n",
       " 9.75,\n",
       " 7.153846153846154,\n",
       " 8.090909090909092,\n",
       " 9.272727272727273,\n",
       " 6.0,\n",
       " 6.8125,\n",
       " 8.538461538461538,\n",
       " 7.642857142857143,\n",
       " 7.2727272727272725,\n",
       " 10.857142857142858,\n",
       " 9.0,\n",
       " 8.615384615384615,\n",
       " 8.3,\n",
       " 7.3,\n",
       " 9.23076923076923,\n",
       " 9.285714285714286,\n",
       " 9.25,\n",
       " 7.8,\n",
       " 7.111111111111111,\n",
       " 8.692307692307692,\n",
       " 8.333333333333334,\n",
       " 10.0,\n",
       " 7.9,\n",
       " 7.0,\n",
       " 7.3076923076923075,\n",
       " 11.444444444444445,\n",
       " 7.071428571428571,\n",
       " 9.0,\n",
       " 7.416666666666667,\n",
       " 7.357142857142857,\n",
       " 6.0,\n",
       " 9.636363636363637,\n",
       " 10.083333333333334,\n",
       " 8.454545454545455,\n",
       " 8.625,\n",
       " 7.583333333333333,\n",
       " 6.2631578947368425,\n",
       " 8.6,\n",
       " 11.333333333333334,\n",
       " 6.117647058823529,\n",
       " 9.076923076923077,\n",
       " 7.75,\n",
       " 7.6923076923076925,\n",
       " 8.0,\n",
       " 7.285714285714286,\n",
       " 7.625,\n",
       " 7.285714285714286,\n",
       " 8.777777777777779,\n",
       " 9.615384615384615,\n",
       " 8.0,\n",
       " 8.11111111111111,\n",
       " 8.416666666666666,\n",
       " 7.333333333333333,\n",
       " 8.615384615384615,\n",
       " 6.769230769230769,\n",
       " 9.4,\n",
       " 9.083333333333334,\n",
       " 10.11111111111111,\n",
       " 5.9,\n",
       " 6.0625,\n",
       " 7.555555555555555,\n",
       " 5.25,\n",
       " 7.888888888888889,\n",
       " 9.2,\n",
       " 7.3076923076923075,\n",
       " 8.333333333333334,\n",
       " 8.9,\n",
       " 5.75,\n",
       " 7.071428571428571,\n",
       " 6.357142857142857,\n",
       " 10.88888888888889,\n",
       " 6.083333333333333,\n",
       " 7.214285714285714,\n",
       " 6.571428571428571,\n",
       " 7.857142857142857,\n",
       " 8.076923076923077,\n",
       " 7.923076923076923,\n",
       " 10.0,\n",
       " 9.416666666666666,\n",
       " 9.5,\n",
       " 7.0,\n",
       " 9.555555555555555,\n",
       " 10.5,\n",
       " 8.23076923076923,\n",
       " 10.8,\n",
       " 9.2,\n",
       " 8.555555555555555,\n",
       " 7.384615384615385,\n",
       " 11.6,\n",
       " 9.5,\n",
       " 8.666666666666666,\n",
       " 9.166666666666666,\n",
       " 8.0,\n",
       " 9.1,\n",
       " 10.5,\n",
       " 6.625,\n",
       " 4.9375,\n",
       " 7.066666666666666,\n",
       " 6.142857142857143,\n",
       " 6.142857142857143,\n",
       " 8.727272727272727,\n",
       " 6.857142857142857,\n",
       " 7.5,\n",
       " 6.416666666666667,\n",
       " 9.23076923076923,\n",
       " 5.923076923076923,\n",
       " 8.545454545454545,\n",
       " 10.818181818181818,\n",
       " 6.571428571428571,\n",
       " 8.428571428571429,\n",
       " 7.142857142857143,\n",
       " 7.066666666666666,\n",
       " 7.066666666666666,\n",
       " 7.0,\n",
       " 10.3,\n",
       " 8.416666666666666,\n",
       " 7.818181818181818,\n",
       " 7.071428571428571,\n",
       " 6.5,\n",
       " 8.6,\n",
       " 6.625,\n",
       " 8.5,\n",
       " 6.5,\n",
       " 9.0,\n",
       " 8.090909090909092,\n",
       " 9.75,\n",
       " 8.777777777777779,\n",
       " 9.666666666666666,\n",
       " 7.416666666666667,\n",
       " 10.25,\n",
       " 8.4,\n",
       " 9.5,\n",
       " 9.153846153846153,\n",
       " 13.0,\n",
       " 8.777777777777779,\n",
       " 7.714285714285714,\n",
       " 11.625,\n",
       " 8.25,\n",
       " 10.166666666666666,\n",
       " 8.461538461538462,\n",
       " 7.5,\n",
       " 8.875,\n",
       " 10.75,\n",
       " 8.444444444444445,\n",
       " 8.909090909090908,\n",
       " 11.555555555555555,\n",
       " 7.818181818181818,\n",
       " 13.25,\n",
       " 7.7,\n",
       " 8.5,\n",
       " 7.0,\n",
       " 9.090909090909092,\n",
       " 5.875,\n",
       " 9.272727272727273,\n",
       " 8.333333333333334,\n",
       " 8.125,\n",
       " 10.11111111111111,\n",
       " 10.090909090909092,\n",
       " 9.1,\n",
       " 9.333333333333334,\n",
       " 10.11111111111111,\n",
       " 11.571428571428571,\n",
       " 8.153846153846153,\n",
       " 5.5,\n",
       " 8.9,\n",
       " 11.833333333333334,\n",
       " 8.0,\n",
       " 8.071428571428571,\n",
       " 15.666666666666666,\n",
       " 9.5,\n",
       " 11.444444444444445,\n",
       " 8.363636363636363,\n",
       " 8.76923076923077,\n",
       " 8.1,\n",
       " 9.25,\n",
       " 10.2,\n",
       " 8.555555555555555,\n",
       " 7.6923076923076925,\n",
       " 10.5,\n",
       " 8.2,\n",
       " 9.0,\n",
       " 8.583333333333334,\n",
       " 6.8,\n",
       " 10.5,\n",
       " 9.090909090909092,\n",
       " 9.1,\n",
       " 7.454545454545454,\n",
       " 7.846153846153846,\n",
       " 5.181818181818182,\n",
       " 9.727272727272727,\n",
       " 8.4,\n",
       " 29.25,\n",
       " 8.076923076923077,\n",
       " 8.846153846153847,\n",
       " 8.090909090909092,\n",
       " 6.8,\n",
       " 7.533333333333333,\n",
       " 9.454545454545455,\n",
       " 9.454545454545455,\n",
       " 9.416666666666666,\n",
       " 7.769230769230769,\n",
       " 9.181818181818182,\n",
       " 7.533333333333333,\n",
       " 7.538461538461538,\n",
       " 7.545454545454546,\n",
       " 7.0,\n",
       " 8.9,\n",
       " 8.75,\n",
       " 8.666666666666666,\n",
       " 9.333333333333334,\n",
       " 8.071428571428571,\n",
       " 8.833333333333334,\n",
       " 8.923076923076923,\n",
       " 7.5,\n",
       " 9.166666666666666,\n",
       " 7.5,\n",
       " 5.894736842105263,\n",
       " 8.5,\n",
       " 7.4,\n",
       " 6.857142857142857,\n",
       " 8.181818181818182,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 5.222222222222222,\n",
       " 7.083333333333333,\n",
       " 9.583333333333334,\n",
       " 8.0,\n",
       " 5.777777777777778,\n",
       " 7.923076923076923,\n",
       " 9.88888888888889,\n",
       " 8.909090909090908,\n",
       " 10.0,\n",
       " 8.333333333333334,\n",
       " 7.357142857142857,\n",
       " 6.666666666666667,\n",
       " 7.090909090909091,\n",
       " 7.666666666666667,\n",
       " 6.9,\n",
       " 8.923076923076923,\n",
       " 6.0,\n",
       " 8.166666666666666,\n",
       " 8.181818181818182,\n",
       " 7.166666666666667,\n",
       " 8.1,\n",
       " 8.0,\n",
       " 8.11111111111111,\n",
       " 7.214285714285714,\n",
       " 7.1,\n",
       " 10.6,\n",
       " 8.076923076923077,\n",
       " 9.4,\n",
       " 7.538461538461538,\n",
       " 10.181818181818182,\n",
       " 8.538461538461538,\n",
       " 9.125,\n",
       " 6.583333333333333,\n",
       " 7.769230769230769,\n",
       " 5.8,\n",
       " 5.277777777777778,\n",
       " 6.1875,\n",
       " 6.0,\n",
       " 7.461538461538462,\n",
       " 8.75,\n",
       " 8.181818181818182,\n",
       " 10.454545454545455,\n",
       " 8.5,\n",
       " 6.769230769230769,\n",
       " 5.615384615384615,\n",
       " 6.6,\n",
       " 8.727272727272727,\n",
       " 7.2727272727272725,\n",
       " 7.923076923076923,\n",
       " 5.6923076923076925,\n",
       " 10.88888888888889,\n",
       " 6.066666666666666,\n",
       " 7.8,\n",
       " 7.7272727272727275,\n",
       " 6.2727272727272725,\n",
       " 8.666666666666666,\n",
       " 6.8125,\n",
       " 12.0,\n",
       " 6.333333333333333,\n",
       " 9.272727272727273,\n",
       " 6.916666666666667,\n",
       " 9.142857142857142,\n",
       " 9.714285714285714,\n",
       " 9.416666666666666,\n",
       " 7.75,\n",
       " 5.833333333333333,\n",
       " 10.7,\n",
       " 5.153846153846154,\n",
       " 7.538461538461538,\n",
       " 6.642857142857143,\n",
       " 6.5,\n",
       " 7.333333333333333,\n",
       " 10.0,\n",
       " 8.615384615384615,\n",
       " 7.583333333333333,\n",
       " 9.083333333333334,\n",
       " 9.0,\n",
       " 7.076923076923077,\n",
       " 7.625,\n",
       " 8.272727272727273,\n",
       " 7.454545454545454,\n",
       " 9.6,\n",
       " 10.25,\n",
       " 5.764705882352941,\n",
       " 6.866666666666666,\n",
       " 10.272727272727273,\n",
       " 8.0,\n",
       " 7.6923076923076925,\n",
       " 8.666666666666666,\n",
       " 11.166666666666666,\n",
       " 6.928571428571429,\n",
       " 10.6,\n",
       " 7.25,\n",
       " 6.545454545454546,\n",
       " 13.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 7.0,\n",
       " 10.8,\n",
       " 15.0,\n",
       " 8.571428571428571,\n",
       " 13.8,\n",
       " 9.75,\n",
       " 9.833333333333334,\n",
       " 7.0,\n",
       " 14.6,\n",
       " 10.75,\n",
       " 11.666666666666666,\n",
       " 15.333333333333334,\n",
       " 6.0,\n",
       " 10.125,\n",
       " 12.5,\n",
       " 10.375,\n",
       " 7.428571428571429,\n",
       " 9.833333333333334,\n",
       " 10.714285714285714,\n",
       " 9.7,\n",
       " 6.6,\n",
       " 10.8,\n",
       " 8.818181818181818,\n",
       " 8.818181818181818,\n",
       " 5.533333333333333,\n",
       " 7.0,\n",
       " 12.11111111111111,\n",
       " 10.5,\n",
       " 9.6,\n",
       " 7.636363636363637,\n",
       " 9.666666666666666,\n",
       " 12.714285714285714,\n",
       " 11.166666666666666,\n",
       " 8.272727272727273,\n",
       " 3.6666666666666665,\n",
       " 8.0,\n",
       " 9.0,\n",
       " 12.125,\n",
       " 12.285714285714286,\n",
       " 9.0,\n",
       " 4.285714285714286,\n",
       " 15.5,\n",
       " 16.666666666666668,\n",
       " 9.666666666666666,\n",
       " 11.142857142857142,\n",
       " 11.0,\n",
       " 5.888888888888889,\n",
       " 16.666666666666668,\n",
       " 9.0,\n",
       " 8.636363636363637,\n",
       " 11.5,\n",
       " 7.545454545454546,\n",
       " 11.7,\n",
       " 10.5,\n",
       " 10.285714285714286,\n",
       " 10.090909090909092,\n",
       " 8.333333333333334,\n",
       " 9.555555555555555,\n",
       " 10.6,\n",
       " 7.833333333333333,\n",
       " 7.818181818181818,\n",
       " 7.142857142857143,\n",
       " 9.083333333333334,\n",
       " 10.2,\n",
       " 12.5,\n",
       " 16.0,\n",
       " 13.0,\n",
       " 9.545454545454545,\n",
       " 8.214285714285714,\n",
       " 18.0,\n",
       " 8.25,\n",
       " 11.75,\n",
       " 10.857142857142858,\n",
       " 13.6,\n",
       " 11.2,\n",
       " 11.4,\n",
       " 13.166666666666666,\n",
       " 13.0,\n",
       " 8.142857142857142,\n",
       " 15.333333333333334,\n",
       " 6.75,\n",
       " 7.142857142857143,\n",
       " 10.875,\n",
       " 10.5,\n",
       " 11.666666666666666,\n",
       " 12.5,\n",
       " 11.25,\n",
       " 8.384615384615385,\n",
       " 10.285714285714286,\n",
       " 9.0,\n",
       " 13.0,\n",
       " 9.916666666666666,\n",
       " 11.4,\n",
       " 10.555555555555555,\n",
       " 7.7272727272727275,\n",
       " 6.3076923076923075,\n",
       " 8.0,\n",
       " 10.181818181818182,\n",
       " 8.7,\n",
       " 10.416666666666666,\n",
       " 6.666666666666667,\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_char = []\n",
    "for tweet in tweet_tokens:\n",
    "    avg_char.append(tweet_counts(tweet)[2])\n",
    "print('The average number of characters per word for each tweet is listed below:')\n",
    "avg_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of words for 12284 number of tweets is: 10.627971344838814\n"
     ]
    }
   ],
   "source": [
    "print('The average number of words for',  df_input.shape[0], 'number of tweets is:', len(all_words)/df_input.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>Average number and standard deviation of characters per token</H4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of characters per token is: 7.363673269298528\n",
      "Standard deviation of characters per token is: 5.036157329254341\n"
     ]
    }
   ],
   "source": [
    "tokens_len = []\n",
    "for token in all_words:\n",
    "    tokens_len.append(len(token))\n",
    "print('Average number of characters per token is:', sum(tokens_len)/len(tokens_len))\n",
    "print('Standard deviation of characters per token is:', statistics.stdev(tokens_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>Total number of tokens corresponding to the top 10 most frequent words (types) in the vocabulary</H4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('trump', 880), ('like', 492), ('get', 361), ('people', 361), ('one', 331), ('new', 307), (\"i'm\", 307), ('us', 294), ('2', 290), ('would', 261)]\n"
     ]
    }
   ],
   "source": [
    "c = Counter(all_words)\n",
    "most_occur = c.most_common(10)\n",
    "print(most_occur) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>The token/type ratio in the dataset</H4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token/type ratio in the dataset is: 0.2703862003462169\n"
     ]
    }
   ],
   "source": [
    "print('The token/type ratio in the dataset is:', len(distinct_words)/len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>The total number of distinct n-grams (of words) that appear in the dataset for n=2,3,4,5</H4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of distinct n-grams (of words) that appear in the dataset for n=2 is: 101748\n",
      "Total number of distinct n-grams (of words) that appear in the dataset for n=3 is: 102408\n",
      "Total number of distinct n-grams (of words) that appear in the dataset for n=4 is: 92664\n",
      "Total number of distinct n-grams (of words) that appear in the dataset for n=5 is: 81242\n"
     ]
    }
   ],
   "source": [
    "def ngram_counts(tweet_list):\n",
    "    bigram_counter = Counter()\n",
    "    trigram_counter = Counter()\n",
    "    fourgram_counter = Counter()\n",
    "    fivegram_counter = Counter()\n",
    "    \n",
    "    for tweet in tweet_list:\n",
    "        bigrams = list(nltk.ngrams(tweet, 2))\n",
    "        trigrams = list(nltk.ngrams(tweet, 3))\n",
    "        fourgrams = list(nltk.ngrams(tweet, 4))\n",
    "        fivegrams = list(nltk.ngrams(tweet, 5))\n",
    "\n",
    "        bigram_counter.update(bigrams)\n",
    "        trigram_counter.update(trigrams)\n",
    "        fourgram_counter.update(fourgrams)\n",
    "        fivegram_counter.update(fivegrams)\n",
    "        \n",
    "        # distinct counts\n",
    "        distinct_bigram_counter = len(bigram_counter) \n",
    "        distinct_trigram_counter = len(trigram_counter) \n",
    "        distinct_fourgram_counter = len(fourgram_counter) \n",
    "        distinct_fivegram_counter = len(fivegram_counter) \n",
    "        \n",
    "    return distinct_bigram_counter, distinct_trigram_counter, distinct_fourgram_counter, distinct_fivegram_counter\n",
    "\n",
    "print(\"Total number of distinct n-grams (of words) that appear in the dataset for n=2 is:\", ngram_counts(tweet_tokens)[0])\n",
    "print(\"Total number of distinct n-grams (of words) that appear in the dataset for n=3 is:\", ngram_counts(tweet_tokens)[1])\n",
    "print(\"Total number of distinct n-grams (of words) that appear in the dataset for n=4 is:\", ngram_counts(tweet_tokens)[2])\n",
    "print(\"Total number of distinct n-grams (of words) that appear in the dataset for n=5 is:\", ngram_counts(tweet_tokens)[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>Token log frequency</H4>\n",
    "<br>According to Zipf's law, the frequency of any word is inversely proportional to its rank in the frequency table</br>\n",
    "<br>If the graph approximately accurate, it should look linear</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def token_log_freq(corpus):\n",
    "    corpus_counts = Counter(corpus)\n",
    "    plt.loglog([val for word, val in corpus_counts.most_common(1000)])\n",
    "    plt.xlabel('rank')\n",
    "    plt.ylabel('frequency')\n",
    "    plt.show()\n",
    "    \n",
    "token_log_freq(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Gold Dev EDA part</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>What is the number of types that appear in the dev data but not the training data (OOV)</H4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Common types in two lists\n",
    "def intersection(lst1, lst2): \n",
    "    return list(set(lst1) & set(lst2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokens_combined = []\n",
    "corpus_combined = []\n",
    "for tweet in df['text']:\n",
    "    tokens = nltk.casual_tokenize(tweet)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    tokens = [token for token in tokens if not token in stop_words]\n",
    "    tokens = [token for token in tokens if not token in punctuation]\n",
    "    tweet_tokens_combined.append(tokens)\n",
    "    corpus_combined.extend(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#common tokens in dev and test folder\n",
    "all_words_combined = []\n",
    "for tweet in tweet_tokens_combined:\n",
    "    for word in tweet:\n",
    "        all_words_combined.append(word.lower())\n",
    "        \n",
    "all_words = []\n",
    "for tweet in tweet_tokens:\n",
    "    for word in tweet:\n",
    "        all_words.append(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_common = intersection(all_words, all_words_combined)\n",
    "number_not_inDev = len(all_words) - len(words_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 119233 number of words that appear in the dev data but not the training data (OOV)\n"
     ]
    }
   ],
   "source": [
    "print('There are',number_not_inDev,'number of words that appear in the dev data but not the training data (OOV)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>Plot vocabulary growth at difference sample sizes N</H4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combined data (training dataset)\n",
    "combined_25 = tweet_tokens_combined[0:int(0.25*len(tweet_tokens_combined))]\n",
    "combined_50 = tweet_tokens_combined[0:int(0.50*len(tweet_tokens_combined))]\n",
    "combined_75 = tweet_tokens_combined[0:int(0.75*len(tweet_tokens_combined))]\n",
    "combined_100 = tweet_tokens_combined\n",
    "\n",
    "#dev data (input.txt) \n",
    "dev_25 = tweet_tokens[0:int(0.25*len(tweet_tokens))]\n",
    "dev_50 = tweet_tokens[0:int(0.50*len(tweet_tokens))]\n",
    "dev_75 = tweet_tokens[0:int(0.75*len(tweet_tokens))]\n",
    "dev_100 = tweet_tokens\n",
    "\n",
    "def getUniqueLen(lst):\n",
    "    all_words = []\n",
    "    for tweet in lst:\n",
    "        for word in tweet:\n",
    "            all_words.append(word.lower())\n",
    "    return (len(set(all_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "types_number_input = []\n",
    "types_number_combined = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "types_number_input.append(getUniqueLen(dev_25))\n",
    "types_number_input.append(getUniqueLen(dev_50))\n",
    "types_number_input.append(getUniqueLen(dev_75))\n",
    "types_number_input.append(getUniqueLen(dev_100))\n",
    "types_number_combined.append(getUniqueLen(combined_25))\n",
    "types_number_combined.append(getUniqueLen(combined_50))\n",
    "types_number_combined.append(getUniqueLen(combined_75))\n",
    "types_number_combined.append(getUniqueLen(combined_100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Vocabulary Growth for Dev and Gold from 25 to 100 percent')"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xlist=[0.25,0.5,0.75,1.0]\n",
    "plt.plot(xlist, types_number_input,color='green', marker='o', linestyle='dashed', label='Dev')\n",
    "plt.plot(xlist, types_number_combined, color='red', marker='o', linestyle='dashed', label='Gold')\n",
    "plt.legend()\n",
    "plt.title(\"Vocabulary Growth for Dev and Gold from 25 to 100 percent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>What is the class distribution of the training data set - how many negative, neutral, positive tweets?</H4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive tweets in gold dataset:  11990\n",
      "Number of neutral tweets in gold dataset:  13831\n",
      "Number of negative tweets in gold dataset:  4810\n"
     ]
    }
   ],
   "source": [
    "# Feature extraction by Polarity\n",
    "\n",
    "#Training Set\n",
    "df_pos_gold = df[df['label'] == 'positive']\n",
    "\n",
    "df_neg_gold = df[df['label'] == 'negative']\n",
    "\n",
    "df_neutral_gold = df[df['label'] == 'neutral']\n",
    "\n",
    "print('Number of positive tweets in gold dataset: ',len(df_pos_gold))\n",
    "print('Number of neutral tweets in gold dataset: ',len(df_neutral_gold))\n",
    "print('Number of negative tweets in gold dataset: ',len(df_neg_gold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>Look at the difference between the top word types across these three classes</H4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_pos = []\n",
    "tweet_neg = []\n",
    "tweet_neu = []\n",
    "\n",
    "for tweet in df_pos_gold['text']:\n",
    "    tokens = nltk.casual_tokenize(tweet)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    tokens = [token for token in tokens if not token in stop_words]\n",
    "    tokens = [token for token in tokens if not token in punctuation]\n",
    "    tweet_pos.append(tokens)\n",
    "\n",
    "for tweet in df_neg_gold['text']:\n",
    "    tokens = nltk.casual_tokenize(tweet)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    tokens = [token for token in tokens if not token in stop_words]\n",
    "    tokens = [token for token in tokens if not token in punctuation]\n",
    "    tweet_neg.append(tokens)\n",
    "\n",
    "for tweet in df_neutral_gold['text']:\n",
    "    tokens = nltk.casual_tokenize(tweet)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    tokens = [token for token in tokens if not token in stop_words]\n",
    "    tokens = [token for token in tokens if not token in punctuation]\n",
    "    tweet_neu.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words = []\n",
    "neg_words = []\n",
    "neu_words = []\n",
    "\n",
    "for tweet in tweet_pos:\n",
    "    for word in tweet:\n",
    "        pos_words.append(word.lower())\n",
    "        \n",
    "for tweet in tweet_neg:\n",
    "    for word in tweet:\n",
    "        neg_words.append(word.lower())\n",
    "        \n",
    "for tweet in tweet_neu:\n",
    "    for word in tweet:\n",
    "        neu_words.append(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get top 10 frequent words in list\n",
    "def top10(lst):\n",
    "    common_gold = Counter(lst) #counter for word in gold dataset\n",
    "    y = OrderedDict(common_gold.most_common(10)) #top10\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in positive tweets:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('tomorrow', 2209),\n",
       "             ('may', 1734),\n",
       "             ('day', 1477),\n",
       "             ('see', 1085),\n",
       "             (\"i'm\", 953),\n",
       "             ('friday', 897),\n",
       "             ('night', 850),\n",
       "             ('going', 835),\n",
       "             ('sunday', 695),\n",
       "             ('time', 694)])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Top 10 words in positive tweets:')\n",
    "top10(pos_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in negative tweets:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('may', 1277),\n",
       "             ('tomorrow', 460),\n",
       "             ('like', 413),\n",
       "             ('1st', 358),\n",
       "             ('day', 293),\n",
       "             (\"i'm\", 276),\n",
       "             ('going', 236),\n",
       "             ('get', 227),\n",
       "             ('time', 222),\n",
       "             ('one', 218)])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Top 10 words in negative tweets:')\n",
    "top10(neg_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in neutral tweets:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('may', 2458),\n",
       "             ('tomorrow', 1641),\n",
       "             ('1st', 860),\n",
       "             ('like', 765),\n",
       "             ('day', 760),\n",
       "             ('sunday', 749),\n",
       "             ('friday', 728),\n",
       "             ('going', 727),\n",
       "             ('night', 691),\n",
       "             ('time', 661)])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Top 10 words in neutral tweets:')\n",
    "top10(neu_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> Based on those three outputs, the top 10 words in three different categories have highly similarity, except words: 'friday', 'sunday', and 'night' don't show up in dataset with negative label </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4> What words are particularly characteristic of your training set and dev set? Are they the same?</H4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in training tweets:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('may', 5469),\n",
       "             ('tomorrow', 4310),\n",
       "             ('day', 2530),\n",
       "             ('see', 1885),\n",
       "             ('1st', 1872),\n",
       "             (\"i'm\", 1853),\n",
       "             ('friday', 1829),\n",
       "             ('going', 1798),\n",
       "             ('like', 1783),\n",
       "             ('night', 1664)])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words_training = []\n",
    "for tweet in combined_100:\n",
    "    for word in tweet:\n",
    "        all_words_training.append(word.lower())\n",
    "\n",
    "print('Top 10 words in training tweets:')\n",
    "top10(all_words_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in dev tweets:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('trump', 880),\n",
       "             ('like', 492),\n",
       "             ('get', 361),\n",
       "             ('people', 361),\n",
       "             ('one', 331),\n",
       "             ('new', 307),\n",
       "             (\"i'm\", 307),\n",
       "             ('us', 294),\n",
       "             ('2', 290),\n",
       "             ('would', 261)])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words_dev = []\n",
    "for tweet in dev_100:\n",
    "    for word in tweet:\n",
    "        all_words_dev.append(word.lower())\n",
    "\n",
    "print('Top 10 words in dev tweets:')\n",
    "top10(all_words_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Tweets in the gold dataset are more like daily posts from the general public. It mentions words such as 'Friday' and 'Night'. It looks like people posting their daily life or plans. The top common word in the dev data are works like 'trump' and 'people'. It looks more like political posts talking about their comments or view on election or policy.</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Part 2: Tweet Data Message Polarity Tests</h3>\n",
    "<br>From the EDA part above, you should have some general information about the tweets dataset, from here, I will explore and run sentiment analysis of these tweets by classifying the labels of each tweet</br>\n",
    "<br>Three classifiers are used:</br>\n",
    "\n",
    "- Naive Bayes\n",
    "- Logistic Regression\n",
    "- Vader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>Additional Cleaning</H4>\n",
    "<br>Since there are some irrelevant information such as hashtags, address signs and so on, more cleaning steps will be needed before fitting the data into different classifiers</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction by Polarity\n",
    "\n",
    "#Training Set\n",
    "df_pos_train = df_train[df_train['label'] == 'positive']\n",
    "pos_tweets = df_pos_train['text'].tolist()\n",
    "\n",
    "df_neg_train = df_train[df_train['label'] == 'negative']\n",
    "neg_tweets = df_neg_train['text'].tolist()\n",
    "\n",
    "df_neutral_train = df_train[df_train['label'] == 'neutral']\n",
    "neutral_tweets = df_neutral_train['text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For tweets, there are potentially number of challenges for tokenization:\n",
    "- mentions/usernames\n",
    "- URLs, numbers\n",
    "- textual [emoticons](https://en.wikipedia.org/wiki/List_of_emoticons)\n",
    "- [emoji](https://en.wikipedia.org/wiki/Emoji)\n",
    "- words (including hyphenated words)\n",
    "- case-folding \n",
    "- punctuation \n",
    "- hashtags \n",
    "- non-English words\n",
    "\n",
    "Therefore, the following code helps preprocess the data to remove the irrelevant information above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process text\n",
    "# English words\n",
    "words = set(nltk.corpus.words.words())\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "ps = PorterStemmer() \n",
    "\n",
    "def cleanTweet(tweetList):\n",
    "    tokenList = []\n",
    "    \n",
    "    # for each tweet\n",
    "    for tweet in tweetList:\n",
    "        \n",
    "        # NLTK tokenizer for Tweets\n",
    "        tokens = nltk.casual_tokenize(tweet)\n",
    "        \n",
    "        # punctuation List\n",
    "        punctuation = list(string.punctuation)\n",
    "\n",
    "        # remove stopwords\n",
    "        tokens = [term.lower() for term in tokens if term.lower() not in stopwords.words('english')]\n",
    "\n",
    "        # remove punctuation\n",
    "        tokens = [term for term in tokens if term not in punctuation]\n",
    "        \n",
    "        # remove web-link (https)\n",
    "        tokens = [term for term in tokens if not term.startswith('http')]\n",
    "        \n",
    "        # remove hashtags\n",
    "        tokens = [term for term in tokens if not term.startswith('#')]\n",
    "\n",
    "        # remove profiles\n",
    "        tokens = [term for term in tokens if not term.startswith('@')]\n",
    "        \n",
    "        # lemminization\n",
    "        tokens = [lemmatizer.lemmatize(term) for term in tokens]\n",
    "        \n",
    "        # stemming words\n",
    "        tokens = [ps.stem(term) for term in tokens]\n",
    "        \n",
    "        tokenList.append(tokens)\n",
    "        \n",
    "    return(tokenList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize (clean) the tweets\n",
    "pos_tweets_tokens =  cleanTweet(pos_tweets)\n",
    "neg_tweets_tokens = cleanTweet(neg_tweets)\n",
    "neutral_tweets_tokens = cleanTweet(neutral_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize (clean) the testing tweets\n",
    "test_tweet = list(df_test['text'])\n",
    "test_tweet_tokens = cleanTweet(test_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>Naive Bayes Classifier</H4>\n",
    "<br>For Naive Bayes method, text data need to be converted ti</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create dictionary for words contained in each tweet with respect to different sentiments\n",
    "def features(tokens):\n",
    "    return dict(('contains(%s)' % w, True) for w in tokens)\n",
    "\n",
    "# create word feature sets for positive/neutral/negative\n",
    "positive_featuresets = [(features(tweet),'positive') for tweet in pos_tweets_tokens]\n",
    "negative_featuresets = [(features(tweet),'negative') for tweet in neg_tweets_tokens]\n",
    "neutral_featuresets = [(features(tweet),'neutral') for tweet in neutral_tweets_tokens]\n",
    "\n",
    "# combine all\n",
    "training_features = positive_featuresets + negative_featuresets + neutral_featuresets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier\n"
     ]
    }
   ],
   "source": [
    "# build NaiveBayes Classifier\n",
    "sentiment_analyzer = SentimentAnalyzer()\n",
    "trainer = NaiveBayesClassifier.train\n",
    "\n",
    "# train on training set\n",
    "classifier = sentiment_analyzer.train(trainer, training_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create evaluation data for testing set\n",
    "df_dev = pd.DataFrame({'text':test_tweet_tokens, 'label':df_test['label']})\n",
    "truth_list = list(df_dev[['text', 'label']].itertuples(index=False, name=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating NaiveBayesClassifier results...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 0.5933420365535248,\n",
       " 'Precision [neutral]': 0.6618357487922706,\n",
       " 'Recall [neutral]': 0.49458483754512633,\n",
       " 'F-measure [neutral]': 0.5661157024793388,\n",
       " 'Precision [negative]': 0.3917963224893918,\n",
       " 'Recall [negative]': 0.5995670995670995,\n",
       " 'F-measure [negative]': 0.47390932420872534,\n",
       " 'Precision [positive]': 0.6475037821482602,\n",
       " 'Recall [positive]': 0.7033689400164339,\n",
       " 'F-measure [positive]': 0.6742812130760142}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict on testing set\n",
    "for i, (text, expected) in enumerate(truth_list):\n",
    "    text_feats = features(text)\n",
    "    truth_list[i] = (text_feats, expected)\n",
    "\n",
    "sentiment_analyzer.evaluate(truth_list,classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating NaiveBayesClassifier results...\n",
      "Accuracy: 0.581266318537859\n",
      "F-measure [negative]: 0.4873949579831932\n",
      "F-measure [neutral]: 0.5467156262749898\n",
      "F-measure [positive]: 0.6602332127060716\n",
      "Precision [negative]: 0.4027777777777778\n",
      "Precision [neutral]: 0.6350710900473934\n",
      "Precision [positive]: 0.6369278510473235\n",
      "Recall [negative]: 0.6170212765957447\n",
      "Recall [neutral]: 0.4799426934097421\n",
      "Recall [positive]: 0.6853088480801336\n"
     ]
    }
   ],
   "source": [
    "# individual metrics\n",
    "for key,value in sorted(sentiment_analyzer.evaluate(truth_list).items()):\n",
    "    print('{0}: {1}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Bigram with Naive Bayes</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhoumengzhi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#Try bigram with Naive Bayes\n",
    "def get_ngrams(text):\n",
    "    n_grams = ngrams(text, 2)\n",
    "    return [ ' '.join(grams) for grams in n_grams]\n",
    "\n",
    "bigram_pos = []\n",
    "for tweet in pos_tweets_tokens:\n",
    "    bigrams = get_ngrams(tweet)\n",
    "    bigram_pos.append(bigrams)\n",
    "    \n",
    "bigram_neg = []\n",
    "for tweet in neg_tweets_tokens:\n",
    "    bigrams = get_ngrams(tweet)\n",
    "    bigram_neg.append(bigrams)\n",
    "    \n",
    "bigram_neu = []\n",
    "for tweet in neutral_tweets_tokens:\n",
    "    bigrams = get_ngrams(tweet)\n",
    "    bigram_neu.append(bigrams)\n",
    "    \n",
    "bigram_test = []\n",
    "for tweet in test_tweet_tokens:\n",
    "    bigrams = get_ngrams(tweet)\n",
    "    bigram_test.append(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_featuresets = [(features(tweet),'positive') for tweet in bigram_pos]\n",
    "negative_featuresets = [(features(tweet),'negative') for tweet in bigram_neg]\n",
    "neutral_featuresets = [(features(tweet),'neutral') for tweet in bigram_neu]\n",
    "training_features = positive_featuresets + negative_featuresets + neutral_featuresets\n",
    "df_dev = pd.DataFrame({'text':bigram_test, 'label':df_test['label']})\n",
    "truth_list = list(df_dev[['text', 'label']].itertuples(index=False, name=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier\n"
     ]
    }
   ],
   "source": [
    "sentiment_analyzer = SentimentAnalyzer()\n",
    "trainer = NaiveBayesClassifier.train\n",
    "classifier = sentiment_analyzer.train(trainer, training_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating NaiveBayesClassifier results...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 0.514686684073107,\n",
       " 'Precision [negative]': 0.29545454545454547,\n",
       " 'Recall [negative]': 0.5255319148936171,\n",
       " 'F-measure [negative]': 0.37825421133231235,\n",
       " 'Precision [positive]': 0.6034341782502044,\n",
       " 'Recall [positive]': 0.6160267111853088,\n",
       " 'F-measure [positive]': 0.6096654275092938,\n",
       " 'Precision [neutral]': 0.5890547263681593,\n",
       " 'Recall [neutral]': 0.42406876790830944,\n",
       " 'F-measure [neutral]': 0.493127863390254}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, (text, expected) in enumerate(truth_list):\n",
    "    text_feats = features(text)\n",
    "    truth_list[i] = (text_feats, expected)\n",
    "\n",
    "sentiment_analyzer.evaluate(truth_list,classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating NaiveBayesClassifier results...\n",
      "Accuracy: 0.514686684073107\n",
      "F-measure [negative]: 0.37825421133231235\n",
      "F-measure [neutral]: 0.493127863390254\n",
      "F-measure [positive]: 0.6096654275092938\n",
      "Precision [negative]: 0.29545454545454547\n",
      "Precision [neutral]: 0.5890547263681593\n",
      "Precision [positive]: 0.6034341782502044\n",
      "Recall [negative]: 0.5255319148936171\n",
      "Recall [neutral]: 0.42406876790830944\n",
      "Recall [positive]: 0.6160267111853088\n"
     ]
    }
   ],
   "source": [
    "# individual metrics\n",
    "for key,value in sorted(sentiment_analyzer.evaluate(truth_list).items()):\n",
    "    print('{0}: {1}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>Logistic Regression</H4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test/training set\n",
    "X_train = df_train['text'].tolist()\n",
    "X_test = df_test['text'].tolist()\n",
    "y_train = df_train['label'].tolist()\n",
    "y_test = df_test['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean tweets based on pre-defined rules\n",
    "X_train_clean = cleanTweet(X_train)\n",
    "for i in range(len(X_train_clean)):\n",
    "    X_train_clean[i] = ' '.join(X_train_clean[i])\n",
    "X_test_clean = cleanTweet(X_test)\n",
    "for i in range(len(X_test_clean)):\n",
    "    X_test_clean[i] = ' '.join(X_test_clean[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhoumengzhi/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/zhoumengzhi/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/zhoumengzhi/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/zhoumengzhi/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/zhoumengzhi/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/zhoumengzhi/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/zhoumengzhi/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/zhoumengzhi/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/zhoumengzhi/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/zhoumengzhi/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/zhoumengzhi/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...       random_state=0, refit=True, scoring=None, solver='lbfgs',\n",
       "           tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build logistic models\n",
    "logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegressionCV(cv=10, random_state=0, multi_class='multinomial')),\n",
    "               ])\n",
    "\n",
    "# train on cross-validation sets\n",
    "logreg.fit(X_train_clean, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6259791122715405\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      0.39      0.46       470\n",
      "     neutral       0.61      0.68      0.64      1396\n",
      "    positive       0.66      0.65      0.66      1198\n",
      "\n",
      "   micro avg       0.63      0.63      0.63      3064\n",
      "   macro avg       0.61      0.57      0.59      3064\n",
      "weighted avg       0.62      0.63      0.62      3064\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predict on testing sets\n",
    "y_pred = logreg.predict(X_test_clean)\n",
    "\n",
    "# Get accuracy and classification report\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>Vader Classifier</H4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up vader sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "#update parameter in model\n",
    "new_words = {\n",
    "     'stoked': 2.0,\n",
    "     'lame': -3.4,\n",
    " }\n",
    "\n",
    "analyzer.lexicon.update(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning for Vader\n",
    "# create dictionary for punctuation/lemmatization/stemming\n",
    "punctuation = list(string.punctuation)\n",
    "\n",
    "# process text\n",
    "def cleandata(text):\n",
    "    # casual tokenize process\n",
    "    tokens = nltk.casual_tokenize(text)\n",
    "\n",
    "    # remove stopwords\n",
    "    tokens = [term.lower() for term in tokens if term.lower() not in stopwords.words('english')]\n",
    "\n",
    "    # remove punctuation\n",
    "    tokens = [term for term in tokens if term not in punctuation]\n",
    "\n",
    "    # remove hashtags\n",
    "    tokens = [term for term in tokens if not term.startswith('#')]\n",
    "\n",
    "    # remove profiles\n",
    "    tokens = [term for term in tokens if not term.startswith('@')]\n",
    "    \n",
    "    # remove website links\n",
    "    tokens = [term for term in tokens if not term.startswith('http')]\n",
    "    \n",
    "\n",
    "    #join tokenizer together\n",
    "    sentence = ' '.join(tokens)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up function for predicting labels\n",
    "def accuracy(data):\n",
    "    label = []\n",
    "    for i in data['text']:\n",
    "        a = cleandata(i)\n",
    "        vs = analyzer.polarity_scores(i)\n",
    "        if vs['compound'] >= 0.05:\n",
    "            label.append('positive')\n",
    "        elif  vs['compound'] <= -0.05:\n",
    "            label.append('negative')\n",
    "        else: \n",
    "            label.append('neutral')\n",
    "    \n",
    "    # get accuracy scores\n",
    "    acc = accuracy_score(data['label'], label)\n",
    "    print(classification_report(data['label'], label, target_names=['positive','negative','neutral']))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.38      0.59      0.46      4340\n",
      "    negative       0.59      0.37      0.45     12435\n",
      "     neutral       0.54      0.65      0.59     10792\n",
      "\n",
      "   micro avg       0.51      0.51      0.51     27567\n",
      "   macro avg       0.50      0.54      0.50     27567\n",
      "weighted avg       0.54      0.51      0.51     27567\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5137664598977038"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Part 3: Tweet Data Performance Evaluation</h3>\n",
    "<br>After go through Part 2: Tweet Data Message Polarity, all required three classifiers has been build based on tweet data. This part mainly focus on comparision between those three models and give further insight from three models performance. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Measure | Naive-Bayes | Logistic Regression | Vader |\n",
    "| --- | --- | --- | --- |\n",
    "| Accuracy | 0.52 | 0.64 | 0.54 |\n",
    "| Weighted F-mesure | 0.50 | 0.64 | 0.51 |\n",
    "| Average Recall | 0.53 | 0.60 | 0.54 |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Performance Comparision: </h4>\n",
    "<br>The table attached above shows the comparision between three different algorithms' performance. And details for each model's precision and recall are showed below. Look through those information, Logistic Regression classifier dominant in all three evaluation metrics: accuracy, F-measure and avgrec. So we use logistic regression model for our final data prediction. Vader is ranked as 2nd highest accuracy among those three models, and Naive-Bayes' performance is the lowest. The potential possible reason is that data dictionary is not large enough for Naive-Bayes classifier in this case. </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> 1) Naive Bayes Detail Performance: </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating NaiveBayesClassifier results...\n",
      "Accuracy: 0.514686684073107\n",
      "F-measure [negative]: 0.37825421133231235\n",
      "F-measure [neutral]: 0.493127863390254\n",
      "F-measure [positive]: 0.6096654275092938\n",
      "Precision [negative]: 0.29545454545454547\n",
      "Precision [neutral]: 0.5890547263681593\n",
      "Precision [positive]: 0.6034341782502044\n",
      "Recall [negative]: 0.5255319148936171\n",
      "Recall [neutral]: 0.42406876790830944\n",
      "Recall [positive]: 0.6160267111853088\n"
     ]
    }
   ],
   "source": [
    "for key,value in sorted(sentiment_analyzer.evaluate(truth_list).items()):\n",
    "    print('{0}: {1}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> 2) Logistic Regression Detail Performance </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6259791122715405\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      0.39      0.46       470\n",
      "     neutral       0.61      0.68      0.64      1396\n",
      "    positive       0.66      0.65      0.66      1198\n",
      "\n",
      "   micro avg       0.63      0.63      0.63      3064\n",
      "   macro avg       0.61      0.57      0.59      3064\n",
      "weighted avg       0.62      0.63      0.62      3064\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = logreg.predict(X_test_clean)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> 3) Vader Detail Performance </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.38      0.59      0.46      4340\n",
      "    negative       0.59      0.37      0.45     12435\n",
      "     neutral       0.54      0.65      0.59     10792\n",
      "\n",
      "   micro avg       0.51      0.51      0.51     27567\n",
      "   macro avg       0.50      0.54      0.50     27567\n",
      "weighted avg       0.54      0.51      0.51     27567\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5137664598977038"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> F-measure and AvgRec: </h4>\n",
    "<br> \n",
    "Both of F-measure and Average Recall are used in performance comparision. There is a main difference between two methods. F-measure use both of precision and recall to calculate accuracy of model. This evaluation metric can be weighted to favor precision or recall. However, Average Recall only counts on recall for positive, negative and neutral which won't be affected by precision. Therefore, Average Recall only consider about how many correct labels was captured by model. </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "for i in df_test['text']:\n",
    "    a = cleandata(i)\n",
    "    vs = analyzer.polarity_scores(i)\n",
    "    if vs['compound'] >= 0.05:\n",
    "        label.append('positive')\n",
    "    elif  vs['compound'] <= -0.05:\n",
    "        label.append('negative')\n",
    "    else: \n",
    "        label.append('neutral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = pd.DataFrame({'original':df_test['label'],'vader':label,'logistic regression':y_pred.tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>vader</th>\n",
       "      <th>logistic regression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23870</th>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19131</th>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22574</th>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12232</th>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20893</th>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       original     vader logistic regression\n",
       "23870   neutral  positive            positive\n",
       "19131   neutral   neutral             neutral\n",
       "22574   neutral   neutral             neutral\n",
       "12232   neutral  negative             neutral\n",
       "20893  positive  positive             neutral"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23870    \"Free Yoga in the Park - Date: September 6, 20...\n",
       "19131    Gucci trousers atman conviction may bring up t...\n",
       "22574    \"Jennings, being used in higher leverage situa...\n",
       "12232    \"Want to go the cinema on my day off tomorrow,...\n",
       "20893    \"No doubt, George Osborne dreams Daniel Craig ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Free Yoga in the Park - Date: September 6, 2015 Yoga classes are free and designed for all experience levels. Part... http://t.co/p0K2eL6hfb\"'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.iloc[0][0] #1st row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Want to go the cinema on my day off tomorrow, but Inside Out or Ant-Man? Will IO be full of kids? So torn right now.\"'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.iloc[3][0] #4th row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"No doubt, George Osborne dreams Daniel Craig will play him in the biopic. All (dyed) Lego hair + macho black jackets. http://t.co/cUhDgR8tD0\"'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.iloc[4][0] #5th row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Mis-performance of classifiers: </h4>\n",
    "<br>\n",
    "For logistic regression, we noticed that the recall score for negative is relatively low compared to other classes. Looking into the problem, we noticed that the training data is quite an imbalance. We have 10791 positive cases and 12461 Neutral cases, but only 4315 negative cases. After training, the logistic model only returns 1574 negative cases, which is only about 10% of the test data. The imbalance of data might be a factor that causing the model to only predict very few negative cases.\n",
    "</br>\n",
    "<br></br>\n",
    "<br> From the given example, 1st, 4th and 5th rows are predicted wrongly by either vader classifier or logistic regression. For the first row, both of two methods have bad performance because of word with multiple meaning: 'free', and it also presents as positive attribute generally. For the fourth row, vader give negative label to this sentence due to its power to understand potential meaning of sentence, and that also cause problem for vader which is too sensitive for emotion. For the fifth row, logistic regression gives neutral label for it and the reason is that logistic regression can't get the potential meaning from that sentence. In contrast, vader is good at it. In general, both of classifiers could not handle word with multiple meaning, but vader is good at classify sentences with potential emotion. Logistic Regression can't deal with potential emotion and also avoid problem of vader has: too sensitive for emotional words. In that way, vader is good at predict sentence with emotion, and logistic regression is good at predict neutral sentence.\n",
    "</br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Importance of Feature Extraction and Tokenization:</h4>\n",
    "<br> Feature extraction and tokenization both are important to sentiment analysis, especially for Naive-Bayes. Both of method could increase accuracy for model. Feature extraction could help classifier distinguish words' attribute: positive, neutral, or negative; also it helps before calculating the word frequency in order to format the correct input for Naive Bayes. It would give direct assistance for model training. Tokenization is required for Naive-Bayes since this kind of classifier need to build word dictionary for classification tasks. Tokenization is also helpful in data cleaning which will reduce noise in dataset. Combination of these two methods could help classifiers finish job successfully and quickly. </br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Potential Improvement: </h4>\n",
    "<br> If we have more time, we would try more different combination of data preprocessing. We could customized tokenizer to focus on social media words or symbol. For exmaple, emoji and uppercase could have strong emotion and is useful in vader sentiment analysis. \"I LOVE ITTTT!!!!üòÅüòÅüòÅüòÅüòÅ\" has stronger positive emotion than \"i love it\". But our tokenizer will try to simplify sentence as much as it can. In this case, it will reduce accurcy for vader. Furthermore, normalization also could be done for data preprocessing. We could edit distribution of token frequency which will help classifier to increase accuracy. We also faced some unexpected challenges in this project. Since vader model has already be established by developer, there is little way to improve accuracy for vader sentiment analysis. The cross validation on Naive-Bayes model also didn't increase accuracy. The accuracy of models only can increase to 64%. Therefore, we can't say our models are well-build. We need to find more creative way to reduce noise and increase accuracy.\n",
    "\n",
    "For logistic regression, we will also consider normalizing the data. This includes reducing counts for keywords that have high frequency and increasing counts for keywords that have a low frequency. By smoothing the distribution of keyword counts, the model won‚Äôt be too sensitive to not meaningful keywords.\n",
    "</br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
